[Unit]
Description=Llama Server - {display_name}
After=network.target

[Service]
Type=simple
WorkingDirectory={working_dir}
Environment="MODEL_PATH={models_dir}/{model_file}"
ExecStartPre=/bin/sh -c 'find /dev/shm -name "llama-server-*.log" -mmin +10 -delete || true'
ExecStart=/usr/local/bin/llama-server \
    -m ${{MODEL_PATH}} \
    --port {port} \
    --host 0.0.0.0 \
    -c {context_size} \
    --n-gpu-layers 99 \
    --metrics \
    --log-file /dev/shm/llama-server-{model_name}.log
Restart=on-failure
RestartSec=10
StandardOutput=null
StandardError=null

[Install]
WantedBy=multi-user.target
